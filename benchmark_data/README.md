# Benchmark Data for TSFM vs Baselines

Unified benchmark comparing TSFM against LiMU-BERT, MOMENT, CrossHAR, LanHAR, and LLaSA
on 20 HAR datasets (10 training + 10 zero-shot test).

## Quick Start

```bash
# 1. Convert all datasets to standardized session format (if not done already)
python datascripts/setup_all_ts_datasets.py

# 2. Export raw per-subject CSVs from TSFM session data
python benchmark_data/scripts/export_raw.py

# 3. Generate 20Hz .npy files for baselines (LiMU-BERT, MOMENT, CrossHAR, LanHAR, LLaSA)
python benchmark_data/scripts/preprocess_limubert.py

# 4. Generate native-rate .npy files for TSFM evaluation
python benchmark_data/scripts/preprocess_tsfm_eval.py
```

## Folder Structure

```
benchmark_data/
├── README.md                   # This file
├── dataset_config.json         # Dataset metadata, train/test splits, channel mappings
├── raw/                        # Standardized per-subject CSVs (generated by export_raw.py)
│   ├── uci_har/
│   │   ├── metadata.json
│   │   ├── subject_01.csv
│   │   └── ...
│   └── ... (20 datasets)
├── processed/
│   ├── tsfm/                   # Symlinks to data/{dataset} (for training)
│   ├── tsfm_eval/              # Native-rate .npy files for TSFM evaluation
│   │   ├── motionsense/
│   │   │   ├── data_native.npy
│   │   │   ├── label_native.npy
│   │   │   └── metadata.json
│   │   └── ... (7 test datasets)
│   ├── limubert/               # 20Hz .npy format (used by ALL baselines)
│   │   ├── uci_har/
│   │   │   ├── data_20_120.npy
│   │   │   ├── label_20_120.npy
│   │   │   └── mapping.json
│   │   ├── global_label_mapping.json  # 87 global labels shared across all training datasets
│   │   └── ... (20 datasets)
│   ├── lanhar/                 # LanHAR text descriptions
│   └── crosshar/               # CrossHAR processed data
└── scripts/
    ├── export_raw.py           # Step 1: session data → per-subject CSVs
    ├── preprocess_limubert.py  # Step 2: CSVs → 20Hz .npy files
    ├── preprocess_tsfm_eval.py # Step 3: session data → native-rate .npy files
    └── preprocess_tsfm.py      # Creates symlinks for TSFM training
```

## Datasets

### Training (10 datasets)

| Dataset | Sessions | Activities | Hz | Channels | Placement |
|---------|----------|------------|-----|----------|-----------|
| UCI-HAR | 10,299 | 6 | 50 | 9 (acc+gyro+total_acc) | Waist |
| HHAR | 15,000* | 6 | 50 | 6 (acc+gyro) | Pocket |
| PAMAP2 | 4,342 | 12 | 100 | 52 (multi-IMU) | Multi-body |
| WISDM | 15,000* | 18 | 20 | 12 (phone+watch) | Pocket+Wrist |
| DSADS | 9,120 | 19 | 25 | 9 (acc+gyro+mag) | Wrist |
| KU-HAR | 17,374 | 17 | 100 | 6 (acc+gyro) | Wrist |
| UniMiB-SHAR | 11,771 | 17 | 50 | 3 (acc only) | Pocket |
| HAPT | 2,546 | 12 | 50 | 6 (acc+gyro) | Waist |
| MHEALTH | 2,029 | 12 | 50 | 23 (multi+ECG) | Multi-body |
| RecGym | 7,150 | 11 | 20 | 6 (acc+gyro) | Chest |

*Subsampled from original count via stratified sampling.

### Zero-Shot Test (7 main + 3 additional)

**Main test datasets** (evaluated by all models):

| Dataset | Windows | Activities | Hz | Difficulty |
|---------|---------|------------|-----|------------|
| MotionSense | 12,080 | 6 | 50 | Easy |
| RealWorld | 27,138 | 8 | 50 | Medium |
| MobiAct | 4,345 | 13 | 50 | Hard |
| VTT-ConIoT | 2,058 | 16 | 50 | Severe (50% coverage) |
| Shoaib | 5,537 | 7 | 50 | Medium |
| Opportunity | 6,453 | 4 | 30 | Medium |
| HARTH | 47,330 | 12 | 50 | Hard (distribution shift) |

**Additional test datasets** (defined in config, not all evaluated yet):

| Dataset | Activities | Hz | Notes |
|---------|------------|-----|-------|
| RealDisp | 33 | 50 | Very high class count |
| Daphnet FOG | 2 | 64 | Freezing of gait (binary) |
| USC-HAD | 12 | 100 | Multi-device |

## Raw CSV Format

Each `subject_XX.csv` contains continuous sensor data per subject:

```csv
timestamp_sec,acc_x,acc_y,acc_z,gyro_x,gyro_y,gyro_z,activity
0.0000,0.120000,-0.030000,0.150000,0.001000,-0.002000,0.003000,walking
0.0200,0.150000,-0.020000,0.180000,0.002000,-0.001000,0.002000,walking
```

**Columns:**
- `timestamp_sec`: Seconds from start of recording (continuous)
- `acc_x`, `acc_y`, `acc_z`: Accelerometer (core, always present)
- `gyro_x`, `gyro_y`, `gyro_z`: Gyroscope (core, present except UniMiB-SHAR)
- Additional dataset-specific channels as extra columns (e.g., `mag_x`, `total_acc_x`)
- `activity`: Activity label string

**Notes:**
- Sessions are concatenated per subject, sorted by session name
- Timestamps are rebuilt as continuous (no gaps between sessions)
- Channel names are standardized: `acc_x/y/z` and `gyro_x/y/z` are always the core channels
- Original dataset-specific names are mapped to these standard names (see `dataset_config.json`)

## LIMU-BERT Format (Used by All Baselines)

Each dataset produces:
- `data_20_120.npy`: Shape `(N, 120, 6)` — float32
  - 6 channels: [acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z]
  - Resampled to 20Hz, 120-sample windows (6 seconds)
  - Non-overlapping windows (stride = 120 samples)
  - UniMiB-SHAR: gyro channels are zero-padded
- `label_20_120.npy`: Shape `(N, 120, 2)` — int32
  - Per-timestep labels (same label repeated across window for uniform-activity windows)
  - Column 0: activity index (see `mapping.json`)
  - Column 1: subject index (see `mapping.json`)
- `mapping.json`: Activity and subject index mappings

## TSFM Evaluation Format

Each test dataset produces:
- `data_native.npy`: Shape `(N, W, 6)` — float32 at native sampling rate
  - W = sampling_rate × 6 seconds (e.g., 300 for 50Hz, 180 for 30Hz)
- `label_native.npy`: Shape `(N, W, 2)` — same format as LIMU-BERT labels
- `metadata.json`: Contains `sampling_rate_hz` and `window_size`

## Configuration

`dataset_config.json` is the authoritative metadata file for all 20 datasets. It defines:
- `train_datasets`: List of 10 training dataset names
- `zero_shot_datasets`: List of 10 test dataset names
- Per-dataset: `activities`, `core_channels` mapping, `sampling_rate_hz`, `num_sessions`
- `subsampling`: Rules for large datasets (HHAR, WISDM → 15K sessions)

## Train/Test Splits

**Training datasets** (10): Used for TSFM pretraining and baseline ZS classifier training.
All 10 training datasets share 87 unique activity labels mapped via `global_label_mapping.json`.

**Zero-shot test datasets** (7+): Never seen during any model's pretraining. For supervised
evaluations, each evaluation script applies random window-level 80/10/10 splits internally
(seed 3431, not subject-based).
