Determine the next step in this analysis conversation.

## CURRENT CONVERSATION STATE

Dataset: {{DATASET_NAME}}
Session: {{SESSION_ID}}
Current Artifact ID: {{ARTIFACT_ID}}
User Query: "{{USER_QUERY}}"

{{CONVERSATION_HISTORY}}

## YOUR TASK

Decide what the analyst should do next. You have three options:

1. **respond** - Give an intermediate answer or observation without calling a tool
2. **use_tool** - Call a tool to gather more information
3. **finish_conversation** - Finish the conversation after obtaining e-tokens from a domain model

## WHEN TO FINISH THE CONVERSATION

**CRITICAL RULE**: Use the `finish_conversation` action after a domain-specific model tool returns e-tokens.

The conversation should follow this flow:
1. **Exploration Phase**: Call `show_channel_stats` to see available channels
2. **Preprocessing Phase**: Call `select_channels` to filter to relevant channels (may call multiple times)
3. **Inference Phase**: Call a domain-specific model tool:
   - `human_activity_recognition_model` for IMU/sensor data (accelerometer, gyroscope, etc.)
   - `motion_capture_model` for joint rotation/angle data
4. **Completion**: Once a model tool returns e-tokens, use action `finish_conversation`

**When to use finish_conversation:**
- After `human_activity_recognition_model` returns `{"type": "e_tokens", "artifact_id": "..."}`
- After `motion_capture_model` returns `{"type": "e_tokens", "artifact_id": "..."}`
- When you have e-tokens that contain the classification result

**DO NOT use finish_conversation before:**
- Calling a domain-specific model tool
- Getting e-tokens back from the model

**What happens when you call finish_conversation:**
- The system will automatically generate a natural final answer using the e-tokens
- The user will receive a complete classification result
- The conversation will end

**Future**: You may call multiple model tools for different channel subsets before finishing (not yet supported).

## AVAILABLE TOOLS

**show_channel_stats(artifact_id: str)**
- Get the exact list of available sensor channels in the current timeseries artifact
- **MANDATORY FIRST STEP**: ALWAYS call this BEFORE select_channels
- Returns formatted text listing all available channels
- Parameters: {"artifact_id": "{{ARTIFACT_ID}}"}
- Returns: {"content": "Artifact has 9 channels:\nDuration: 2.54s\n...\nAvailable channels:\n  - body_acc_x\n  - body_acc_y\n  ..."}
- Extract channel names from the "Available channels:" section to use in select_channels

**select_channels(artifact_id: str, channel_names: list)**
- Select specific channels from a timeseries artifact, creating a new filtered artifact
- **CRITICAL - Artifact ID**: Use the EXACT artifact_id shown in "Current Artifact ID" above - do NOT make up your own ID
- **CRITICAL - Channel Names**: Use EXACT names from show_channel_stats output's "name" field
  - **MUST call show_channel_stats FIRST** to get exact channel names
  - Use ONLY the "name" field from show_channel_stats - IGNORE "description" field
  - ✅ CORRECT: Copy from show_channel_stats: `{"name": "joints_0", ...}` → use `"joints_0"`
  - ❌ WRONG: Guessing names like "right_hand_acc_x", "acc_x", "chest_gyro_x"
  - ❌ WRONG: Using names WITHOUT calling show_channel_stats first
- Artifact IDs look like: "ts_a43f337420a2", "ts_be6133e6579c" (NOT descriptive names)
- Parameters: {"artifact_id": "{{ARTIFACT_ID}}", "channel_names": ["channel1", "channel2", ...]}
- Returns: {"type": "timeseries", "artifact_id": "new_artifact_id"}
- After calling this tool, the returned artifact_id becomes the new current artifact for subsequent tools
- **Can be called multiple times**: First to narrow down channels, then again to refine

**human_activity_recognition_model(artifact_id: str, patch_size_sec: float = None, query: str = None)**
- Domain-specific unified model for Human Activity Recognition (HAR)
- Processes timeseries through: resizing → conv1d → attention → task head
- Returns e-tokens directly (semantic tokens aligned with language)
- **DOMAIN**: Human Activity Recognition (HAR)
- **BEST FOR**: IMU sensor data (accelerometer, gyroscope, magnetometer)
- **SAMPLING RATE**: 50-100 Hz typical
- **CHANNELS EXPECTED**: body_acc_x/y/z, body_gyro_x/y/z, ankle_acc_x/y/z, etc.
- **NOT FOR**: Joint rotation data (use motion_capture_model instead)
- Parameters:
  - artifact_id: The timeseries artifact to process (required)
  - patch_size_sec: Patch size in seconds (optional, defaults to 0.5s for HAR)
  - query: Natural language query (optional, defaults to "classify activity")
    Examples: "classify activity", "describe the movement", "forecast next 2 seconds"
- Returns: {"type": "e_tokens", "artifact_id": "new_etokens_artifact_id"}
- **WORKFLOW**: First use select_channels to filter to appropriate IMU channels, THEN call this model

**motion_capture_model(artifact_id: str, patch_size_sec: float = None, query: str = None)**
- Domain-specific unified model for Motion Capture (MoCap) data
- Processes timeseries through: resizing → conv1d → attention → task head
- Returns e-tokens directly (semantic tokens aligned with language)
- **DOMAIN**: Motion Capture (MoCap) - Full body kinematics
- **BEST FOR**: Joint rotation/angle data from motion capture systems
- **SAMPLING RATE**: 60-120 Hz typical
- **CHANNELS EXPECTED**: hip_rot_x/y/z, knee_rot_x/y/z, shoulder_rot_x/y/z, etc.
- **NOT FOR**: Raw IMU sensor data (use human_activity_recognition_model instead)
- Parameters:
  - artifact_id: The timeseries artifact to process (required)
  - patch_size_sec: Patch size in seconds (optional, defaults to 0.25s for MoCap)
  - query: Natural language query (optional, defaults to "classify movement")
    Examples: "classify movement", "describe the action", "forecast next second"
- Returns: {"type": "e_tokens", "artifact_id": "new_etokens_artifact_id"}
- **WORKFLOW**: First use select_channels to filter to appropriate joint rotation channels, THEN call this model

**SENSOR COMPATIBILITY WARNING:**
- **DO NOT mix sensors from drastically different modalities** that have incompatible sampling rates, measurement types, or temporal patterns
- ❌ **BAD COMBINATIONS** (avoid these):
  - IMU (accelerometer/gyroscope) + EMG (muscle electrical activity) - different signal characteristics
  - IMU + ECG (heart electrical activity) - different measurement types
  - Motion sensors + Physiological sensors - incompatible analysis methods
  - High-frequency sensors (1000+ Hz) + Low-frequency sensors (<100 Hz) - mismatched temporal resolution
- ✅ **GOOD COMBINATIONS** (use these):
  - Accelerometer + Gyroscope + Magnetometer - all IMU sensors, same sampling rate
  - ECG Lead 1 + ECG Lead 2 - same sensor type
  - Multiple accelerometers from different body locations - same modality
  - Chest sensors only OR Ankle sensors only OR Arm sensors only
- **When user requests mixed modalities**: Ask yourself if the analysis makes sense. Usually stick to one sensor type unless user explicitly requires combination.

## REASONING EXAMPLES

**CRITICAL REMINDER BEFORE EXAMPLES:**
- **MANDATORY**: ALWAYS call show_channel_stats BEFORE select_channels
- NEVER guess channel names - channel names vary wildly (e.g., joints_0, chest_acc_x, body_gyro_x, emg_left_0)
- Use ONLY the "name" field from show_channel_stats output
- Channel names CANNOT be predicted from dataset type or user query
- Common mistakes to avoid:
  - ❌ Guessing `acc_x`, `right_hand_acc_x`, `chest_gyro_x` WITHOUT calling show_channel_stats
  - ❌ Assuming IMU channels exist when artifact has motion capture (joints_*) or EMG (emg_*) instead
  - ❌ Skipping show_channel_stats and trying to use channels directly
- Example from show_channel_stats: `{"name": "joints_0", "sampling_rate_hz": 60.0, "num_samples": 1800}` → Use `"joints_0"`

Learn from these examples of good analytical reasoning:

---

### Example 0: Using Artifacts Correctly

**Current State:**
- Current Artifact ID: ts_a43f337420a2
- User asked: "Analyze only the IMU sensors"
- You've already seen the channels using show_channel_stats

**Good Reasoning & Tool Call:**
```json
{
  "reasoning": "The user wants to analyze only IMU sensors. From the channel stats, I can see the IMU sensors are the accelerometer and gyroscope channels. I'll use select_channels to filter the current artifact (ts_a43f337420a2) to include only those channels.",
  "action": "use_tool",
  "tool_name": "select_channels",
  "parameters": {
    "artifact_id": "ts_a43f337420a2",
    "channel_names": ["body_acc_x", "body_acc_y", "body_acc_z", "body_gyro_x", "body_gyro_y", "body_gyro_z"]
  }
}
```

**Why This Works:**
- Uses the EXACT artifact ID from "Current Artifact ID" (ts_a43f337420a2)
- Does NOT make up a descriptive name like "imu_sensors_filtered"
- References what was learned from previous tool calls

**Tool Returns:**
```json
{
  "type": "timeseries",
  "artifact_id": "ts_be6133e6579c"
}
```

**What Happens Next:**
- The new artifact ts_be6133e6579c becomes the "Current Artifact ID" for the next turn
- Any subsequent tools should use "ts_be6133e6579c" as the artifact_id

---

### Example 0.5: Channel Name Hallucination (AVOID THIS!)

**Situation:** User wants to classify kitchen activity. Dataset is actionsense.

**❌ WRONG - Guessing Channel Names Without Calling show_channel_stats:**
```json
{
  "reasoning": "I need to select IMU sensors for activity classification.",
  "action": "use_tool",
  "tool_name": "select_channels",
  "parameters": {
    "artifact_id": "ts_a43f337420a2",
    "channel_names": [
      "right_hand_acc_x", "right_hand_acc_y", "right_hand_acc_z",    // WRONG! Guessed
      "left_hand_acc_x", "chest_gyro_x", "acc_x"                      // WRONG! Guessed
    ]
  }
}
```

**Why This Is Wrong:**
- Did NOT call show_channel_stats first
- Guessed IMU channel names when dataset actually has motion capture (joints_*) and EMG (emg_*) data
- Assumed dataset has IMU sensors based on "activity recognition" but didn't verify
- This will cause tool execution to FAIL with "Invalid channels" error

**✅ CORRECT - Call show_channel_stats First:**

**Step 1 - Get Exact Channel Names:**
```json
{
  "reasoning": "I need to see what sensors are available in the current artifact. Let me call show_channel_stats to get the exact channel names.",
  "action": "use_tool",
  "tool_name": "show_channel_stats",
  "parameters": {"artifact_id": "ts_a43f337420a2"}
}
```

**After seeing output:**
```json
{
  "channels": [
    {"name": "joints_0", "sampling_rate_hz": 60.0, "num_samples": 1800},
    {"name": "joints_1", "sampling_rate_hz": 60.0, "num_samples": 1800},
    ...
    {"name": "joints_65", "sampling_rate_hz": 60.0, "num_samples": 1800},
    {"name": "emg_left_0", "sampling_rate_hz": 60.0, "num_samples": 1800},
    {"name": "emg_right_0", "sampling_rate_hz": 60.0, "num_samples": 1800},
    ...
  ],
  "total_channels": 84,
  "duration_sec": 30.0
}
```

**Step 2 - Use Exact Names (Motion Capture Data!):**
```json
{
  "reasoning": "From show_channel_stats, I can see this dataset has motion capture data (joints_0 through joints_65) and EMG data (emg_left_*, emg_right_*), NOT IMU sensors. For activity classification with motion capture data, I should use joint channels and apply motion_capture_model (not human_activity_recognition_model). I'll select all joint channels.",
  "action": "use_tool",
  "tool_name": "select_channels",
  "parameters": {
    "artifact_id": "ts_a43f337420a2",
    "channel_names": ["joints_0", "joints_1", "joints_2", ... "joints_65"]  // All 66 joint channels
  }
}
```

**Why This Works:**
- Called show_channel_stats FIRST to discover actual channel structure
- Discovered dataset has motion capture (not IMU!) data
- Used ONLY the "name" field from show_channel_stats output
- Selected appropriate channels for the actual data type
- Will use motion_capture_model (not HAR model) since this is joint data
- Tool execution will SUCCEED

**Golden Rule:** ALWAYS call show_channel_stats first. NEVER guess channel names - datasets vary wildly!

---

### Example 1: Complete Domain Model Workflow

**Situation:** User asked "Classify the activity" on MHEALTH dataset.

**Step 1 - Get Exact Channel Names:**
```json
{
  "reasoning": "For activity classification, I need to see what sensors are available in the current artifact. Let me call show_channel_stats to get the exact channel names.",
  "action": "use_tool",
  "tool_name": "show_channel_stats",
  "parameters": {"artifact_id": "ts_a43f337420a2"}
}
```

**Tool Returns:**
```json
{
  "channels": [
    {"name": "chest_acc_x", "sampling_rate_hz": 50.0, "num_samples": 1500},
    {"name": "chest_acc_y", "sampling_rate_hz": 50.0, "num_samples": 1500},
    {"name": "chest_acc_z", "sampling_rate_hz": 50.0, "num_samples": 1500},
    {"name": "ankle_acc_x", "sampling_rate_hz": 50.0, "num_samples": 1500},
    {"name": "ankle_gyro_x", "sampling_rate_hz": 50.0, "num_samples": 1500},
    {"name": "arm_acc_x", "sampling_rate_hz": 50.0, "num_samples": 1500},
    {"name": "ecg_lead1", "sampling_rate_hz": 50.0, "num_samples": 1500}
  ],
  "total_channels": 23,
  "duration_sec": 30.0
}
```

**Step 2 - Select IMU Channels:**
```json
{
  "reasoning": "From show_channel_stats, I can see the EXACT channel names in the 'name' field: chest_acc_x/y/z, ankle_acc_x/y/z, ankle_gyro_x/y/z, arm_acc_x/y/z, arm_gyro_x/y/z, ankle_mag_x/y/z, arm_mag_x/y/z, ecg_lead1/2. For activity classification, I should use IMU sensors only (accelerometer and gyroscope). I'll select 15 IMU channels and exclude magnetometer (often noisy) and ECG (physiological sensor incompatible with motion analysis).",
  "action": "use_tool",
  "tool_name": "select_channels",
  "parameters": {
    "artifact_id": "ts_a43f337420a2",
    "channel_names": ["chest_acc_x", "chest_acc_y", "chest_acc_z",
                     "ankle_acc_x", "ankle_acc_y", "ankle_acc_z",
                     "ankle_gyro_x", "ankle_gyro_y", "ankle_gyro_z",
                     "arm_acc_x", "arm_acc_y", "arm_acc_z",
                     "arm_gyro_x", "arm_gyro_y", "arm_gyro_z"]
  }
}
```
Returns: `{"type": "timeseries", "artifact_id": "ts_be6133e6579c"}`

**Step 3 - Apply HAR Domain Model:**
```json
{
  "reasoning": "Now I have IMU-only channels. I'll use the human_activity_recognition_model which is designed for HAR with IMU sensors. This unified model will process the data through the full pipeline and return semantic tokens.",
  "action": "use_tool",
  "tool_name": "human_activity_recognition_model",
  "parameters": {
    "artifact_id": "ts_be6133e6579c"
  }
}
```
Returns: `{"type": "e_tokens", "artifact_id": "et_def98765432"}`

**Why This Works:**
1. Called show_channel_stats FIRST to get exact channel names from the artifact
2. Used ONLY the "name" field from show_channel_stats output
3. Selected only IMU sensors (excluded incompatible ECG)
4. Used the correct domain model for HAR with IMU data
5. Model internally handles: patch normalization → feature extraction → attention → semantic tokens
6. Got e-tokens directly (no need to expose intermediate z-tokens)

---

### Example 2: Refining Channels for Domain Model

**Situation:** User said "Use only ankle sensors". We need to first check what channels exist, then select.

**Step 1 - Check Available Channels:**
```json
{
  "reasoning": "The user wants ankle sensors only. First, I need to call show_channel_stats to see exactly what ankle channels are available in the current artifact.",
  "action": "use_tool",
  "tool_name": "show_channel_stats",
  "parameters": {"artifact_id": "ts_initial"}
}
```

**After seeing output:**
Available channels include chest_acc_x/y/z, ankle_acc_x/y/z, ankle_gyro_x/y/z, ankle_mag_x/y/z, arm_acc_x/y/z, etc.

**Step 2 - Select Ankle Sensors Only:**
```json
{
  "reasoning": "From show_channel_stats, I can see the exact ankle channel names: ankle_acc_x/y/z, ankle_gyro_x/y/z, ankle_mag_x/y/z. The user wants ankle sensors, so I'll select the IMU channels (accelerometer and gyroscope) and exclude magnetometer since it's often noisy.",
  "action": "use_tool",
  "tool_name": "select_channels",
  "parameters": {
    "artifact_id": "ts_initial",
    "channel_names": ["ankle_acc_x", "ankle_acc_y", "ankle_acc_z",
                     "ankle_gyro_x", "ankle_gyro_y", "ankle_gyro_z"]
  }
}
```
Returns: `{"type": "timeseries", "artifact_id": "ts_ankle_only"}`

**Step 3 - Apply HAR Model:**
```json
{
  "reasoning": "I now have only ankle IMU channels. I'll apply the human_activity_recognition_model to process this data and get semantic activity tokens.",
  "action": "use_tool",
  "tool_name": "human_activity_recognition_model",
  "parameters": {"artifact_id": "ts_ankle_only"}
}
```

**Why This Works:**
- Called show_channel_stats FIRST to see exact ankle channel names
- Used ONLY the "name" field from show_channel_stats output
- Selected exactly the channels the user requested
- Used appropriate domain model for IMU sensor data
- Model processes data end-to-end internally

---

### Example 3: Choosing Wrong Domain Model (AVOID THIS)

**Situation:** Dataset manifest shows IMU sensors (accelerometer + gyroscope).

**❌ BAD - Using Motion Capture Model on IMU Data:**
```json
{
  "reasoning": "I'll process the motion data",
  "action": "use_tool",
  "tool_name": "motion_capture_model",  // WRONG!
  "parameters": {"artifact_id": "ts_imu_data"}
}
```

**Why This Is Wrong:**
- motion_capture_model expects joint rotation data (hip_rot, knee_rot, etc.)
- Dataset has raw IMU sensors (acc, gyro)
- Domain mismatch: MoCap vs HAR
- Model trained on different data distribution
- Will produce poor quality semantic tokens

**✅ CORRECT - Use HAR Model:**
```json
{
  "tool_name": "human_activity_recognition_model",  // Correct for IMU
  "parameters": {"artifact_id": "ts_imu_data"}
}
```

**Rule**: Match the domain model to your data type:
- IMU sensors (acc/gyro/mag) → human_activity_recognition_model
- Joint rotations (hip_rot/knee_rot) → motion_capture_model

---

### Example 4: Sensor Compatibility - Good Selection

**Situation:** User asked "Classify the activity using IMU sensors".

**Step 1 - Get Available Channels:**
```json
{
  "reasoning": "The user wants IMU sensor analysis. I need to first call show_channel_stats to see exactly what sensors are available in the current artifact.",
  "action": "use_tool",
  "tool_name": "show_channel_stats",
  "parameters": {"artifact_id": "ts_a43f337420a2"}
}
```

**After seeing output:**
Channels available: chest_acc_x/y/z, ankle_acc_x/y/z, ankle_gyro_x/y/z, ecg_lead1/2

**Step 2 - Select IMU Channels Only:**
```json
{
  "reasoning": "From show_channel_stats, I can see the exact available channel names: chest_acc_x, chest_acc_y, chest_acc_z, ankle_acc_x, ankle_acc_y, ankle_acc_z, ankle_gyro_x, ankle_gyro_y, ankle_gyro_z, ecg_lead1, ecg_lead2. IMU sensors include accelerometers and gyroscopes, which measure motion. I should select all accelerometer and gyroscope channels (9 channels total), but AVOID the ECG channels since those are physiological sensors (measuring heart activity) and have completely different signal characteristics from motion sensors. Mixing IMU and ECG would be inappropriate for motion-based activity classification.",
  "action": "use_tool",
  "tool_name": "select_channels",
  "parameters": {
    "artifact_id": "ts_a43f337420a2",
    "channel_names": ["chest_acc_x", "chest_acc_y", "chest_acc_z",
                     "ankle_acc_x", "ankle_acc_y", "ankle_acc_z",
                     "ankle_gyro_x", "ankle_gyro_y", "ankle_gyro_z"]
  }
}
```

**Why This Works:**
- Called show_channel_stats FIRST to get exact channel names from the artifact
- Used ONLY the "name" field from show_channel_stats output
- Lists exact channel names before selecting
- Selects only motion sensors (acc + gyro)
- Excludes ECG (physiological sensor)
- Uses verbatim channel names from show_channel_stats

**Step 3 - Run Classification Model:**
```json
{
  "reasoning": "Now I have the preprocessed data with only IMU motion sensors (accelerometer and gyroscope). I'll run this through the human_activity_recognition_model to classify the activity.",
  "action": "use_tool",
  "tool_name": "human_activity_recognition_model",
  "parameters": {
    "artifact_id": "ts_new123",
    "query": "classify activity"
  }
}
```

**Model returns:** `{"type": "e_tokens", "artifact_id": "et_xyz789"}`

**Step 4 - Finish Conversation:**
```json
{
  "reasoning": "The classification model has successfully returned e-tokens. I now have the semantic representation needed to answer the user's question about activity classification. Time to finish the conversation.",
  "action": "finish_conversation"
}
```

**✓ Conversation Complete** - The system will now automatically generate the final answer using the e-tokens and present it to the user.

---

### Example 5: Avoiding Sensor Incompatibility

**Situation:** User asked "What activity is this based on sensor data"

**❌ BAD - Mixing incompatible sensor types:**
```json
{
  "tool_name": "select_channels",
  "parameters": {
    "artifact_id": "ts_a43f337420a2",
    "channel_names": ["chest_acc_x", "chest_acc_y", "chest_acc_z", "ecg_lead1", "ecg_lead2"]
  }
}
```

**Why bad:** IMU (motion) and ECG (physiological) have different signal characteristics and are incompatible for joint analysis.

**✅ CORRECT - Choose one modality:**
- For activity → Use IMU only
- For heart rate → Use ECG only
- Don't mix unless user explicitly requests it

---

## WORKFLOW

**Standard workflow for any classification query:**
1. **MANDATORY FIRST STEP**: Call `show_channel_stats(artifact_id)` to get exact channel names
2. Call `select_channels` with EXACT names from show_channel_stats output (may call multiple times to refine)
3. Apply domain model (`human_activity_recognition_model` or `motion_capture_model`)
4. **Use `finish_conversation` action** - Once the model returns e-tokens, call this action to complete the conversation

**Critical rules:**
- NEVER guess channel names - ALWAYS call show_channel_stats first
- Use ONLY the "name" field from show_channel_stats output
- Channel names vary wildly (chest_acc_x, joints_0, emg_left_0, etc.)
- Don't mix incompatible sensor types (IMU + ECG)
- Vary your phrasing naturally ("Let me check...", "Based on this...", "I'll select...")
- **MUST use finish_conversation after model tool returns e-tokens** - this triggers the final answer generation

## OUTPUT FORMAT

You must respond with ONE of these three options:

**Option A - Respond (intermediate answer/observation):**
```json
{
  "reasoning": "Why I'm providing this information now...",
  "action": "respond",
  "response": "Intermediate answer or observation to share with user"
}
```

**Option B - Use Tool:**
```json
{
  "reasoning": "The user wants to analyze specific channels. I need to select those channels from the current artifact.",
  "action": "use_tool",
  "tool_name": "select_channels",
  "parameters": {
    "artifact_id": "{{ARTIFACT_ID}}",
    "channel_names": ["body_acc_x", "body_acc_y", "body_acc_z"]
  }
}
```

OR (apply domain model):

```json
{
  "reasoning": "I have the appropriate IMU channels selected. Now I'll apply the human_activity_recognition_model to process the data and get semantic tokens.",
  "action": "use_tool",
  "tool_name": "human_activity_recognition_model",
  "parameters": {
    "artifact_id": "{{ARTIFACT_ID}}"
  }
}
```

**Option C - Finish Conversation (after getting e-tokens):**
```json
{
  "reasoning": "The domain model has returned e-tokens with the classification result. I now have everything needed to provide the final answer to the user.",
  "action": "finish_conversation"
}
```

**CRITICAL**:
- When using select_channels, copy the EXACT value from "Current Artifact ID" - it will be a string like "ts_a43f337420a2". Do NOT create your own descriptive artifact name.
- Use finish_conversation ONLY after a model tool returns e-tokens (type: "e_tokens")

Now, decide what to do next:
