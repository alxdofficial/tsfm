import os
import random
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import numpy.random as npr
import pandas as pd
import torch
from torch.utils.data import Dataset

DEFAULT_DEBUG_DIR = os.path.join("debug", "datasets", "actionsenseqa", "plots")


@dataclass
class ActionSenseQARecord:
    question: str
    answer: str
    subject: str
    split: str
    activity_index: int
    activity_name: str
    sensor_path: str
    video_path: str
    raw_row: Dict[str, Any]


class ActionSenseQADataset(Dataset):
    """
    Dataset for ActionSense QA pairs generated by qa_generation.py.

    - Loads per-activity sensor CSVs into memory (one per manifest row).
    - Aligns QA pairs to their underlying activity via (subject, split, activity_index).
    - Returns padded sensor patches plus question/answer text and metadata per sample.
    """

    def __init__(
        self,
        base_dir: str = "data/actionsenseqa/data",
        qa_csv_path: str = "data/actionsenseqa/data/qa_pairs.csv",
        manifest_csv_path: str = "data/actionsenseqa/data/manifest.csv",
        split: str = "train",
        val_ratio: float = 0.2,
        split_seed: int = 42,
        patch_size: int = 1600,
        context_size: int = -1,
        max_patches: int = 50,
        log_mode: str = "info",
    ) -> None:
        assert split in {"train", "val"}, "split must be 'train' or 'val'"

        self.base_dir = base_dir
        self.patch_size = int(patch_size)
        self.context_size = int(context_size)
        self.max_patches = int(max_patches)
        self.log_mode = log_mode

        self.manifest_map = self._load_manifest(manifest_csv_path)
        self.all_records = self._load_qa_records(qa_csv_path)

        if not self.all_records:
            raise ValueError(f"No QA records could be loaded from {qa_csv_path}")

        self.records = self._split_records(self.all_records, split, val_ratio, split_seed)

        sensor_paths = sorted({rec.sensor_path for rec in self.records})
        self.sensor_cache = self._load_sensor_cache(sensor_paths)

        missing = [rec.sensor_path for rec in self.records if rec.sensor_path not in self.sensor_cache]
        if missing:
            raise FileNotFoundError(
                "Missing sensor CSVs for samples: " + ", ".join(missing[:5]) +
                (" ..." if len(missing) > 5 else "")
            )

        self._log(
            f"[INFO] ActionSenseQA split={split}: {len(self.records)} QA pairs "
            f"(val_ratio={val_ratio}, seed={split_seed}, patch_size={self.patch_size})",
            level="info",
        )

    # ------------------------------------------------------------------
    # Dataset protocol
    # ------------------------------------------------------------------
    def __len__(self) -> int:
        return len(self.records)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        record = self.records[idx]
        cache_entry = self.sensor_cache.get(record.sensor_path)
        if cache_entry is None:
            raise KeyError(f"Sensor cache missing for path: {record.sensor_path}")

        values = cache_entry["values"]
        timestamps = cache_entry["timestamps"]

        patches, trimmed_ts = self._segment_to_patches(values, timestamps)
        if self.context_size > 0 and patches.shape[0] > self.context_size:
            keep = self.context_size
            patches = patches[-keep:]
            if self.patch_size > 0 and trimmed_ts.size:
                trimmed_ts = trimmed_ts[-keep * self.patch_size:]

        if self.max_patches > 0 and patches.shape[0] > self.max_patches:
            patches = patches[-self.max_patches :]
            if self.patch_size > 0 and trimmed_ts.size:
                trimmed_ts = trimmed_ts[-self.max_patches * self.patch_size :]

        sample = {
            "patches": torch.from_numpy(patches).float(),
            "question": record.question,
            "answer": record.answer,
            "metadata": {
                "subject": record.subject,
                "split": record.split,
                "activity_index": record.activity_index,
                "activity_name": record.activity_name,
                "sensor_path": record.sensor_path,
                "video_path": record.video_path,
                "timestamps": torch.from_numpy(trimmed_ts.astype(np.float64)),
                "columns": cache_entry["columns"],
                "raw_row": record.raw_row,
            },
        }
        return sample

    # ------------------------------------------------------------------
    # Loading helpers
    # ------------------------------------------------------------------
    def _load_manifest(self, manifest_csv_path: str) -> Dict[Tuple[str, str, int], Dict[str, Any]]:
        if not os.path.exists(manifest_csv_path):
            raise FileNotFoundError(f"Manifest not found: {manifest_csv_path}")

        df = pd.read_csv(manifest_csv_path)
        required = {"subject", "split", "activity_index", "csv_path", "activity_name", "video_path"}
        missing = required - set(df.columns)
        if missing:
            raise ValueError(f"Manifest missing required columns: {missing}")

        mapping: Dict[Tuple[str, str, int], Dict[str, Any]] = {}
        for _, row in df.iterrows():
            key = (str(row["subject"]), str(row["split"]), int(row["activity_index"]))
            sensor_path = os.path.join(self.base_dir, str(row["csv_path"]))
            video_path = os.path.join(self.base_dir, str(row["video_path"]))
            mapping[key] = {
                "sensor_path": sensor_path,
                "video_path": video_path,
                "activity_name": row.get("activity_name", ""),
                "t0_abs": row.get("t0_abs"),
                "t1_abs": row.get("t1_abs"),
                "raw": row.to_dict(),
            }
        self._log(f"[INFO] Loaded manifest with {len(mapping)} activities from {manifest_csv_path}", level="info")
        return mapping

    def _load_qa_records(self, qa_csv_path: str) -> List[ActionSenseQARecord]:
        if not os.path.exists(qa_csv_path):
            raise FileNotFoundError(f"QA CSV not found: {qa_csv_path}")

        df = pd.read_csv(qa_csv_path)
        required = {"subject", "split", "activity_index", "question", "answer"}
        missing = required - set(df.columns)
        if missing:
            raise ValueError(f"QA CSV missing required columns: {missing}")

        records: List[ActionSenseQARecord] = []
        skipped = 0
        for _, row in df.iterrows():
            key = (str(row["subject"]), str(row["split"]), int(row["activity_index"]))
            manifest_entry = self.manifest_map.get(key)
            if manifest_entry is None:
                skipped += 1
                continue

            records.append(
                ActionSenseQARecord(
                    question=str(row["question"]),
                    answer=str(row["answer"]),
                    subject=key[0],
                    split=key[1],
                    activity_index=key[2],
                    activity_name=str(manifest_entry.get("activity_name", "")),
                    sensor_path=manifest_entry["sensor_path"],
                    video_path=manifest_entry["video_path"],
                    raw_row=row.to_dict(),
                )
            )

        if skipped:
            self._log(f"[WARN] Skipped {skipped} QA rows with missing manifest entries", level="warn")
        self._log(f"[INFO] Loaded {len(records)} QA rows from {qa_csv_path}", level="info")
        return records

    def _split_records(
        self,
        records: List[ActionSenseQARecord],
        split: str,
        val_ratio: float,
        split_seed: int,
    ) -> List[ActionSenseQARecord]:
        n = len(records)
        rng = npr.RandomState(split_seed)
        indices = np.arange(n)
        rng.shuffle(indices)

        n_val = int(round(val_ratio * n))
        n_val = min(max(n_val, 0), n)

        chosen = indices[n_val:] if split == "train" else indices[:n_val]
        subset = [records[i] for i in chosen]
        self._log(f"[INFO] Split '{split}' selected {len(subset)} / {n} QA rows", level="info")
        return subset

    def _load_sensor_cache(self, sensor_paths: List[str]) -> Dict[str, Dict[str, Any]]:
        cache: Dict[str, Dict[str, Any]] = {}
        for path in sensor_paths:
            if not os.path.exists(path):
                self._log(f"[WARN] Sensor CSV missing: {path}", level="warn")
                continue
            df = pd.read_csv(path)
            if "time_s" not in df.columns:
                raise ValueError(f"Sensor CSV missing 'time_s' column: {path}")

            timestamps = df["time_s"].to_numpy(dtype=np.float64)
            numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c != "time_s"]
            values = df[numeric_cols].to_numpy(dtype=np.float32)
            values = np.nan_to_num(values, nan=0.0, posinf=0.0, neginf=0.0)

            cache[path] = {
                "timestamps": timestamps,
                "values": values,
                "columns": numeric_cols,
            }
        self._log(f"[INFO] Cached sensor data for {len(cache)} segments", level="info")
        return cache

    def _log(self, message: str, level: str = "info") -> None:
        mode = getattr(self, "log_mode", "info")
        if level == "error":
            print(message)
            return
        if level == "warn":
            if mode != "silent":
                print(message)
            return
        if level == "info":
            if mode in {"info", "debug"}:
                print(message)
            return
        if level == "debug" and mode == "debug":
            print(message)

    # ------------------------------------------------------------------
    # Utility
    # ------------------------------------------------------------------
    def _segment_to_patches(
        self,
        segment: np.ndarray,
        timestamps: np.ndarray,
    ) -> Tuple[np.ndarray, np.ndarray]:
        if segment.size == 0:
            raise ValueError("Empty sensor segment cannot be converted to patches")
        if segment.shape[0] != timestamps.shape[0]:
            raise ValueError("Sensor values and timestamps must have identical length")

        if self.patch_size <= 0:
            return segment[np.newaxis, ...], timestamps

        T = segment.shape[0]
        if T < self.patch_size:
            pad = self.patch_size - T
            pad_vals = np.zeros((pad, segment.shape[1]), dtype=segment.dtype)
            pad_ts = np.full((pad,), timestamps[0], dtype=timestamps.dtype)
            segment = np.concatenate([pad_vals, segment], axis=0)
            timestamps = np.concatenate([pad_ts, timestamps], axis=0)
            return segment[np.newaxis, ...], timestamps

        remainder = T % self.patch_size
        if remainder != 0:
            usable = T - remainder
            segment = segment[-usable:]
            timestamps = timestamps[-usable:]

        patches = segment.reshape(-1, self.patch_size, segment.shape[1])
        return patches, timestamps


def actionsenseqa_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
    if len(batch) == 0:
        return {}

    B = len(batch)
    P_list = [item["patches"].shape[0] for item in batch]
    T = batch[0]["patches"].shape[1]
    D = batch[0]["patches"].shape[2]
    P_max = max(P_list)

    patches_out, padmask_out = [], []
    questions, answers = [], []
    metadata_list: Dict[str, List[Any]] = {
        "subject": [],
        "split": [],
        "activity_index": [],
        "activity_name": [],
        "sensor_path": [],
        "video_path": [],
        "timestamps": [],
        "columns": [],
        "raw_row": [],
    }

    for item in batch:
        patches = item["patches"]
        P_i = patches.shape[0]
        pad = P_max - P_i
        if pad > 0:
            pad_tensor = torch.zeros((pad, T, D), dtype=patches.dtype)
            patches_pad = torch.cat([pad_tensor, patches], dim=0)
            mask = torch.cat([torch.zeros(pad, dtype=torch.bool), torch.ones(P_i, dtype=torch.bool)], dim=0)
        else:
            patches_pad = patches
            mask = torch.ones(P_i, dtype=torch.bool)

        patches_out.append(patches_pad)
        padmask_out.append(mask)
        questions.append(item["question"])
        answers.append(item["answer"])

        md = item["metadata"]
        for key in metadata_list:
            metadata_list[key].append(md.get(key))

    return {
        "patches": torch.stack(patches_out, dim=0),
        "pad_mask": torch.stack(padmask_out, dim=0),
        "questions": questions,
        "answers": answers,
        "metadata": metadata_list,
    }


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Debug visualisation for ActionSenseQADataset")
    parser.add_argument("--base_dir", default="data/actionsenseqa/data")
    parser.add_argument("--qa_csv", default="data/actionsenseqa/data/qa_pairs.csv")
    parser.add_argument("--manifest_csv", default="data/actionsenseqa/data/manifest.csv")
    parser.add_argument("--outdir", default=DEFAULT_DEBUG_DIR)
    parser.add_argument("--num", type=int, default=4)
    parser.add_argument("--seed", type=int, default=123)
    parser.add_argument("--patch_size", type=int, default=1600)
    parser.add_argument("--context_size", type=int, default=-1)
    parser.add_argument("--log_mode", choices=["silent", "info", "debug"], default="info")
    args = parser.parse_args()

    random.seed(args.seed)
    np.random.seed(args.seed)

    os.makedirs(args.outdir, exist_ok=True)

    dataset = ActionSenseQADataset(
        base_dir=args.base_dir,
        qa_csv_path=args.qa_csv,
        manifest_csv_path=args.manifest_csv,
        patch_size=args.patch_size,
        context_size=args.context_size,
        log_mode=args.log_mode,
        split="train",
        val_ratio=0.2,
        split_seed=args.seed,
    )

    num_samples = min(args.num, len(dataset))
    indices = random.sample(range(len(dataset)), num_samples)

    for idx in indices:
        sample = dataset[idx]
        metadata = sample["metadata"]
        patches = sample["patches"].numpy()
        timestamps = metadata["timestamps"].numpy()
        columns = metadata["columns"]

        flat = patches.reshape(-1, patches.shape[-1])
        valid_len = min(len(timestamps), flat.shape[0])
        flat = flat[-valid_len:]
        ts = timestamps[-valid_len:]

        plt.figure(figsize=(12, 5))
        for d in range(min(10, flat.shape[1])):
            label = columns[d] if columns and d < len(columns) else f"ch{d}"
            plt.plot(ts, flat[:, d], label=label)

        plt.title(
            f"{metadata['subject']} {metadata['activity_name']}\n"
            f"Q: {sample['question']}\nA: {sample['answer']}"
        )
        plt.xlabel("time_s")
        plt.ylabel("sensor value")
        plt.legend(loc="upper right", ncol=2)
        plt.tight_layout()

        save_path = os.path.join(args.outdir, f"actionsenseqa_sample_{idx}.png")
        plt.savefig(save_path, dpi=120)
        plt.close()

        print(
            f"[DEBUG] Saved {save_path} | subject={metadata['subject']} | "
            f"activity={metadata['activity_name']} | patches={patches.shape}"
        )
