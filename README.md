# IMU Activity Recognition Encoder - Pretraining

**Self-supervised pretraining of a dual-branch transformer encoder for IMU-based human activity recognition across multiple datasets.**

---

## ğŸ¯ What is This?

This project implements a **pretrained encoder** for IMU (Inertial Measurement Unit) time series data that can be fine-tuned for various activity recognition tasks. The encoder uses a **two-stage training pipeline**:

### Stage 1: Self-Supervised Pretraining
1. **Masked Autoencoding (MAE)** - Reconstructs randomly masked sensor patches
2. **Contrastive Learning** - Aligns augmented views of the same data
3. **Dual-Branch Transformer** - Captures both temporal dynamics and cross-sensor relationships

### Stage 2: Semantic Alignment
4. **Text-IMU Alignment** - Aligns IMU embeddings with activity text descriptions
5. **Prototype Learning** - Learns activity prototypes with memory bank
6. **Zero-shot Classification** - Enables classification without fine-tuning

### Key Features

- âœ… **Cross-channel attention**: Models relationships between different sensors (accelerometer â†” gyroscope)
- âœ… **Variable channel support**: Handles 6-52 channels with automatic padding/masking
- âœ… **Multi-dataset pretraining**: Trains on 10 datasets (UCI HAR, HHAR, MHEALTH, PAMAP2, WISDM, UniMiB, DSADS, HAPT, KU-HAR, RecGym)
- âœ… **Semantic alignment**: Align IMU embeddings with natural language descriptions
- âœ… **Multi-prototype learning**: K=3 prototypes per activity class capture intra-class variation
- âœ… **SO(3) rotation augmentation**: Random 3D rotations for sensor orientation invariance
- âœ… **Structured masking**: Span masking + channel dropout for robust Stage 1 pretraining
- âœ… **Temperature-based sampling**: `p_i ~ n_i^0.5` balancing across datasets
- âœ… **Physically-plausible augmentations**: Jitter, time warp, magnitude scaling, channel shuffling, rotation
- âœ… **Mixed precision training**: FP16 with torch.compile for ~4x speedup
- âœ… **Per-dataset tracking**: Monitor learning progress per dataset

---

## ğŸ—ï¸ Architecture Overview

```
Raw IMU Data (variable length, 6-52 channels)
         â†“
    Preprocessing (interpolate to 64-sample patches)
         â†“
    Patch Tokenization (2-second windows â†’ patches)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dual-Branch Transformer Encoder               â”‚
â”‚                                                 â”‚
â”‚  For each of 4 transformer blocks:            â”‚
â”‚  1. Temporal Self-Attention                    â”‚
â”‚     â””â”€ Attention over time (patch dim)        â”‚
â”‚  2. Cross-Channel Self-Attention               â”‚
â”‚     â””â”€ Attention over sensors (channel dim)   â”‚
â”‚  3. Feed-Forward Network                       â”‚
â”‚                                                 â”‚
â”‚  Output: (batch, patches, channels, d_model)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                    â†“
   Projection Head    Reconstruction Head
   (for contrastive)  (for MAE)
         â†“                    â†“
   InfoNCE Loss        MSE Loss
         â†˜                  â†™
        Combined Loss â†’ Backprop
```

---

## ğŸ“‚ Repository Structure

```
tsfm/
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ DATA_FORMAT.md                         # Data format specification
â”œâ”€â”€ requirements.txt                       # Python dependencies
â”‚
â”œâ”€â”€ data/                                  # Datasets (after processing)
â”‚   â”œâ”€â”€ uci_har/                          # UCI HAR dataset
â”‚   â”œâ”€â”€ hhar/                             # HHAR dataset
â”‚   â”œâ”€â”€ mhealth/                          # MHEALTH dataset
â”‚   â”œâ”€â”€ pamap2/                           # PAMAP2 dataset
â”‚   â”œâ”€â”€ wisdm/                            # WISDM dataset
â”‚   â”œâ”€â”€ unimib_shar/                      # UniMiB SHAR dataset
â”‚   â””â”€â”€ motionsense/                      # MotionSense dataset
â”‚
â”œâ”€â”€ datascripts/                          # Dataset download & conversion
â”‚   â”œâ”€â”€ README.md                         # Dataset documentation
â”‚   â”œâ”€â”€ setup_all_datasets.py            # Master pipeline
â”‚   â””â”€â”€ {dataset}/
â”‚       â”œâ”€â”€ download.py                   # Download raw data
â”‚       â””â”€â”€ convert.py                    # Convert to standard format
â”‚
â”œâ”€â”€ datasets/                             # PyTorch dataset classes
â”‚   â””â”€â”€ imu_pretraining_dataset/
â”‚       â”œâ”€â”€ README.md                     # Dataset usage docs
â”‚       â”œâ”€â”€ multi_dataset_loader.py       # Multi-dataset dataloader
â”‚       â””â”€â”€ augmentations.py              # Physical augmentations
â”‚
â”œâ”€â”€ tools/                                # Model implementations
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ imu_activity_recognition_encoder/
â”‚           â”œâ”€â”€ README.md                 # Model documentation
â”‚           â”œâ”€â”€ encoder.py                # Main encoder
â”‚           â”œâ”€â”€ transformer.py            # Dual-branch transformer
â”‚           â”œâ”€â”€ semantic_alignment.py     # Semantic alignment head
â”‚           â”œâ”€â”€ token_text_encoder.py     # Text encoding utilities
â”‚           â”œâ”€â”€ preprocessing.py          # Data preprocessing
â”‚           â”œâ”€â”€ positional_encoding.py    # Position embeddings
â”‚           â””â”€â”€ config.py                 # Model configurations
â”‚
â”œâ”€â”€ training_scripts/                     # Training scripts
â”‚   â””â”€â”€ human_activity_recognition/
â”‚       â”œâ”€â”€ README.md                     # Training documentation
â”‚       â”œâ”€â”€ pretrain.py                   # Stage 1: MAE + Contrastive pretraining
â”‚       â”œâ”€â”€ semantic_alignment_train.py   # Stage 2: Text-IMU alignment
â”‚       â”œâ”€â”€ losses.py                     # MAE + Contrastive losses
â”‚       â”œâ”€â”€ semantic_loss.py              # Semantic alignment losses
â”‚       â””â”€â”€ memory_bank.py                # Prototype memory bank
â”‚
â”œâ”€â”€ val_scripts/                          # Validation and evaluation
â”‚   â””â”€â”€ human_activity_recognition/
â”‚       â”œâ”€â”€ model_loading.py              # Shared model/label bank loading
â”‚       â”œâ”€â”€ eval_config.py                # Shared eval config (patch sizes, datasets)
â”‚       â”œâ”€â”€ evaluate_tsfm.py              # TSFM model evaluation
â”‚       â”œâ”€â”€ compare_models.py             # Model comparison utilities
â”‚       â”œâ”€â”€ benchmark_baselines.py        # Baseline model benchmarks
â”‚       â”œâ”€â”€ evaluation_metrics.py         # Accuracy and metrics
â”‚       â”œâ”€â”€ plot_utils.py                 # Training visualization
â”‚       â””â”€â”€ visualization_3d.py           # Embedding visualization
â”‚
â””â”€â”€ tests/                                # Regression test suite (pytest)
    â”œâ”€â”€ test_model_loading.py             # Model construction & loading
    â”œâ”€â”€ test_encoder_forward.py           # Encoder forward pass & masks
    â”œâ”€â”€ test_similarity_computation.py    # Similarity & metrics
    â”œâ”€â”€ test_losses.py                    # MAE, contrastive, InfoNCE losses
    â”œâ”€â”€ test_augmentations.py             # SO(3) rotation & augmentations
    â”œâ”€â”€ test_memory_bank.py               # Memory bank operations
    â”œâ”€â”€ test_label_groups.py              # Label group mapping
    â””â”€â”€ test_data_loading.py              # Dataset & collation
```

### Baseline Metric Protocol (Updated 2026-02-16)

The baseline evaluation scripts under `val_scripts/human_activity_recognition/` now use:

- Fixed-class macro F1 for closed-set metrics (class list is explicit, even when some classes are absent in a split).
- Ambiguity-safe closed-set label mapping: exact label matches are preferred; group-based mapping is only used when it maps to a single target label.
- Strict 1% supervision for MOMENT-style evaluation (no train+val label-budget inflation).
- Full-dataset benchmark loading in `benchmark_baselines.py` (no default session truncation, no random 70/15/15 slicing in benchmark mode).

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install PyTorch (adjust for your CUDA version)
pip install torch torchvision

# Install other dependencies
pip install -r requirements.txt
```

**Dependencies** (see `requirements.txt`):
- numpy, pandas, matplotlib, pyarrow
- scikit-learn, umap-learn (for evaluation/visualization)
- pydantic (configuration validation)
- google-genai (for text embeddings in Stage 2)

### 2. Download & Process Datasets

```bash
# Download and convert all datasets (~20 minutes, ~2GB)
python datascripts/setup_all_datasets.py

# Or process individually
python datascripts/setup_all_datasets.py uci_har
python datascripts/setup_all_datasets.py hhar
python datascripts/setup_all_datasets.py mhealth
python datascripts/setup_all_datasets.py pamap2
python datascripts/setup_all_datasets.py wisdm
python datascripts/setup_all_datasets.py unimib_shar
python datascripts/setup_all_datasets.py motionsense
```

This downloads raw data, converts to standardized format, and splits into train/val/test.

### 3. Run Pretraining

```bash
cd training_scripts/human_activity_recognition

# Stage 1: MAE + Contrastive pretraining
python pretrain.py

# Or resume from checkpoint
python pretrain.py --resume path/to/checkpoint.pt

# Stage 2: Semantic alignment (after Stage 1)
python semantic_alignment_train.py
```

Training outputs:
```
training_output/imu_pretraining/20250110_143052/
â”œâ”€â”€ config.yaml                 # Saved configuration
â”œâ”€â”€ plots/                      # PNG loss curves
â”‚   â”œâ”€â”€ overall_loss.png
â”‚   â”œâ”€â”€ loss_components.png
â”‚   â”œâ”€â”€ per_dataset_losses.png
â”‚   â”œâ”€â”€ learning_rate.png
â”‚   â”œâ”€â”€ dataset_*_detail.png
â”‚   â””â”€â”€ metrics.json
â”œâ”€â”€ latest.pt                   # Latest checkpoint
â”œâ”€â”€ best.pt                     # Best validation loss
â””â”€â”€ checkpoint_epoch_10.pt      # Periodic checkpoints
```

### 4. Monitor Training

Training automatically generates PNG plots every 10 epochs and at the end of training:

```bash
# Plots are saved to:
training_output/imu_pretraining/{timestamp}/plots/
```

Generated plots:
- `overall_loss.png` - Train/val loss curves
- `loss_components.png` - MAE loss vs Contrastive loss
- `per_dataset_losses.png` - Per-dataset loss curves (UCI HAR, MHEALTH, PAMAP2, WISDM)
- `learning_rate.png` - Learning rate schedule
- `dataset_*_detail.png` - Detailed per-dataset metrics
- `metrics.json` - Raw metrics data

---

## ğŸ“Š Datasets

| Dataset | Channels | Rate | Activities | Description |
|---------|----------|------|------------|-------------|
| **UCI HAR** | 6 (acc+gyro) | 50 Hz | 6 | Smartphone IMU activities |
| **HHAR** | 6 (acc+gyro) | 50-200 Hz | 6 | Heterogeneous HAR (multiple devices) |
| **MHEALTH** | 23 (3 IMUs+ECG) | 50 Hz | 12 | Multi-sensor body activities |
| **PAMAP2** | 40 (3 IMUs+HR) | 100 Hz | 18 | Physical activity monitoring |
| **WISDM** | 6 (phone acc+gyro) | 20 Hz | 18 | Smartphone activities |
| **UniMiB SHAR** | 3 (acc) | 50 Hz | 17 | ADL and falls detection |
| **MotionSense** | 12 (acc+gyro+attitude) | 50 Hz | 6 | iPhone motion data |

All datasets are converted to a standardized format with:
- Variable-length time series
- Consistent channel naming
- Activity labels (not used during pretraining)
- Train/val/test splits

---

## âš™ï¸ Configuration

All hyperparameters are **hard-coded** in `pretrain.py` for easy modification:

```python
# Data
DATASETS = ['uci_har', 'hhar', 'mhealth', 'pamap2', 'wisdm', 'unimib_shar']
PATCH_SIZE_SEC = 2.0  # 2-second patches (varies per dataset)

# Model
D_MODEL = 384
NUM_HEADS = 8
NUM_TEMPORAL_LAYERS = 4
USE_CROSS_CHANNEL = True  # Enable cross-channel attention

# Training
EPOCHS = 100
BATCH_SIZE = 20
LEARNING_RATE = 1e-4
WARMUP_EPOCHS = 10

# Loss
MAE_WEIGHT = 1.0
CONTRASTIVE_WEIGHT = 1.0
TEMPERATURE = 0.2
MASK_RATIO = 0.5  # 50% masking
```

To change hyperparameters, edit the constants at the top of `main()` in `pretrain.py`.

---

## ğŸ§ª Model Details

### Encoder Architecture

- **Input:** Patches of shape (batch, num_patches, 64, num_channels)
- **CNN Feature Extraction:** 1D convolution [kernel=5] â†’ d_model=384
- **Positional Embeddings:** Sinusoidal temporal + SentenceBERT channel semantic
- **Transformer Blocks (Ã—4):**
  - Temporal self-attention (across patches)
  - Cross-channel self-attention (across sensors)
  - Feed-forward network (384 â†’ 1536 â†’ 384)
- **Output:** (batch, num_patches, num_channels, d_model)

### Pretraining Objectives

1. **Masked Autoencoding (MAE):**
   - Randomly mask 50% of patches
   - Reconstruct masked patches from encoded representations
   - Normalized MSE loss (per-patch normalization)

2. **Contrastive Learning (InfoNCE):**
   - Apply augmentations (jitter, scale, time warp, channel shuffle)
   - Maximize agreement between original and augmented views
   - Patch-level contrastive loss across batch

### Augmentations

- **Weak:** jitter, scale, time_shift (preserve semantics)
- **Strong:** time_warp, magnitude_warp, resample (more aggressive)
- **Novel:** channel_shuffle (robustness to channel ordering)
- **SO(3) rotation:** Random 3D rotation applied to sensor triads (acc_x/y/z, gyro_x/y/z). Same rotation matrix for all triads at the same body location. Handles sensor orientation variance across placements.

**See [`datasets/imu_pretraining_dataset/README.md`](datasets/imu_pretraining_dataset/README.md) for augmentation details.**

---

## ğŸ“ˆ Training Tips

### Expected Performance

- Initial loss: ~2-3
- After convergence: ~0.5-1.0
- Training time: 8-12 hours on single GPU (V100/A100)

### Per-Dataset Metrics

Monitor per-dataset losses to identify:
- Which datasets are harder to learn
- Dataset imbalance issues
- Overfitting on specific datasets

### Memory Usage

- With mixed precision (FP16): ~8-12 GB GPU memory
- Batch size 32: ~10 GB
- Reduce batch size if OOM errors occur

### Debugging

If training diverges:
1. Check data loading (run tests in `datasets/imu_pretraining_dataset/`)
2. Verify augmentations aren't too aggressive
3. Reduce learning rate or increase warmup
4. Check for NaN gradients in TensorBoard

---

## ğŸ“ Fine-Tuning (Future Work)

After pretraining, the encoder can be fine-tuned for:

1. **Activity Classification:** Add linear head, fine-tune on labeled data
2. **Activity Detection:** Add segmentation head for temporal localization
3. **Anomaly Detection:** Train classifier on normal data only
4. **Transfer Learning:** Fine-tune on new datasets/activities

The pretrained weights are in `best.pt` under `model_state_dict['encoder']`.

---

## ğŸ”— Key Documents

- **[tools/models/imu_activity_recognition_encoder/README.md](tools/models/imu_activity_recognition_encoder/README.md)** - Model API
- **[datasets/imu_pretraining_dataset/README.md](datasets/imu_pretraining_dataset/README.md)** - Dataset details
- **[training_scripts/human_activity_recognition/README.md](training_scripts/human_activity_recognition/README.md)** - Training details
- **[DATA_FORMAT.md](DATA_FORMAT.md)** - Standardized data format specification

---

## ğŸ› Testing

Run the regression test suite (111 tests):

```bash
# Run all tests
pytest tests/ -v

# Run specific test files
pytest tests/test_losses.py -v
pytest tests/test_encoder_forward.py -v
```

All tests should pass before training or after any refactoring.

---

## ğŸ“ Recent Changes

- âœ… **4 accuracy improvements**: SO(3) rotation augmentation, multi-prototype learning (K=3), structured masking (span + channel dropout), temperature-based sampling (alpha=0.5)
- âœ… **4x training speedup**: Batch fusion, channel bucketing, caching, torch.compile + 6 bug fixes
- âœ… **8 bug fixes**: MAE normalization, memory bank boundary, scheduler resume, channel encoding fallback, and more
- âœ… Added Stage 2 semantic alignment training pipeline
- âœ… Implemented text-IMU alignment with learnable label bank
- âœ… Added memory bank for prototype learning
- âœ… Added group-balanced sampling and patch size augmentation
- âœ… Expanded to 10 training datasets (UCI HAR, HHAR, MHEALTH, PAMAP2, WISDM, UniMiB, DSADS, HAPT, KU-HAR, RecGym)
- âœ… 4 zero-shot test datasets excluded from training (MotionSense, RealWorld, MobiAct, VTT-ConIoT)
- âœ… Added embedding visualization tools (3D, 4D video)
- âœ… Implemented evaluation metrics and model comparison utilities

---

## ğŸ¤ Contributing

When working on this codebase:

1. **Test before committing:** Run all tests to ensure nothing broke
2. **Document changes:** Update relevant README files
3. **Follow conventions:** Use the existing code style
4. **Check for bugs:** Run the bug review before major changes

---

## ğŸ“œ License

[Add your license here]

---

## ğŸ·ï¸ Branch Info

**Branch:** `master`
**Purpose:** IMU activity recognition encoder pretraining + semantic alignment
**Status:** Active development - two-stage training pipeline complete
