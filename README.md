# Tool-Use Time Series Foundation Model (tool-use-om)

**A compositional foundation model system combining frozen tokenizers, lightweight task heads, and LLM orchestration for dataset-agnostic time series analysis.**

---

## ğŸ¯ What is This?

This project implements **Option 2** from our architecture discussions: a three-layer system where:

1. **Frozen pretrained tokenizers** convert raw time series â†’ latent tokens
2. **Lightweight task heads** operate in token space (classification, forecasting, QA)
3. **LLM orchestration** reasons about which models/heads to use and interprets results

This approach provides:
- âœ… **Zero/few-shot** capability via frozen representations
- âœ… **Task flexibility** via interchangeable heads
- âœ… **Dataset generality** via standardized preprocessing
- âœ… **Interpretability** via LLM-driven reasoning and explanations

ğŸ“– **Full architecture details:** See [`ARCHITECTURE.md`](ARCHITECTURE.md)

---

## ğŸ—ï¸ Two-Phase Implementation

We're building this system incrementally:

### Phase 1: EDA Tool-Use Reasoning (Current)

**Goal:** Teach LLM to reason about time series data exploration and preprocessing.

**Status:** âœ… Infrastructure complete, â³ Training data generation in progress

**What's Working:**
- Standardized dataset format (parquet + manifest + labels)
- 5 dataset converters (UCI HAR, PAMAP2, MHEALTH, WISDM, ActionSense)
- 4 EDA tools: session stats, channel stats, channel selection, time filtering
- Debug visualizations for data verification
- Real-world time semantics (seconds, not timesteps)

**Example Query:**
```
User: "How many sessions are longer than 30 seconds? What channels are available?"

LLM Tool Reasoning:
1. show_session_stats("actionsense")
   â†’ 545 sessions, avg 33.1s, min 2.0s, max 178.9s
2. Interprets: ~470 sessions > 30s
3. show_channel_stats("actionsense")
   â†’ 66 joint channels, all at 60Hz
4. Returns: "Approximately 470 sessions exceed 30 seconds. Dataset has 66
   joint motion capture channels sampled at 60Hz, covering full body kinematics."
```

### Phase 2: Model & Head Selection (Planned)

**Goal:** Teach LLM to select and configure tokenizers and task heads.

**Tools to Add:**
- `select_tokenizer` - Choose pretrained encoder
- `configure_tokenizer` - Set patch size, stride, channel subset
- `select_task_head` - Pick classification/forecasting/QA head
- `configure_head` - Set head-specific parameters

**Example Query:**
```
User: "Find walking activities and predict next 5 seconds of accelerometer."

LLM Tool Reasoning:
1. select_tokenizer(task="classification+forecasting", modalities=["accel"])
   â†’ Chooses Model-B (good for forecasting), 2s patches, 0.5s stride
2. select_task_head(task="classification", method="prototypes", labels=5)
   â†’ Few-shot prototype classifier
3. [Executes] â†’ Finds 45 walking segments (92% confidence)
4. select_task_head(task="forecasting", horizon=5.0, channels=["accel_x/y/z"])
   â†’ Latent dynamics + accel decoder
5. [Executes] â†’ Predicts next 5s with Â±0.3 m/sÂ² uncertainty
```

ğŸ“– **Phase 2 design details:** See [`tools/PHASE2_DESIGN.md`](tools/PHASE2_DESIGN.md)

---

## ğŸ—‚ï¸ Repository Structure

```
tsfm/
â”œâ”€â”€ ARCHITECTURE.md              # Full architecture & rationale (READ THIS FIRST)
â”œâ”€â”€ README.md                    # This file
â”‚
â”œâ”€â”€ data/                        # Standardized datasets
â”‚   â”œâ”€â”€ actionsense/
â”‚   â”‚   â”œâ”€â”€ manifest.json        # Minimal metadata (channels, sampling rates)
â”‚   â”‚   â”œâ”€â”€ labels.json          # Session â†’ activity labels
â”‚   â”‚   â”œâ”€â”€ sessions/            # Parquet files per session
â”‚   â”‚   â””â”€â”€ debug_*.png          # Visualization for verification
â”‚   â”œâ”€â”€ uci_har/
â”‚   â”œâ”€â”€ pamap2/
â”‚   â”œâ”€â”€ mhealth/
â”‚   â””â”€â”€ wisdm/
â”‚
â”œâ”€â”€ datascripts/                 # Dataset download & conversion
â”‚   â”œâ”€â”€ README.md                # Dataset pipeline documentation
â”‚   â”œâ”€â”€ download_all_datasets.py
â”‚   â”œâ”€â”€ convert_*.py             # Per-dataset converters
â”‚   â”œâ”€â”€ setup_all_datasets.py   # Master pipeline
â”‚   â””â”€â”€ visualization_utils.py  # Debug plotting
â”‚
â”œâ”€â”€ tools/                       # Tool definitions & executors
â”‚   â”œâ”€â”€ schemas.json             # Phase 1 EDA tool schemas
â”‚   â”œâ”€â”€ executor.py              # Phase 1 tool implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ PHASE2_DESIGN.md         # Phase 2 model selection tools (spec)
â”‚
â””â”€â”€ docs/                        # Design documents
    â”œâ”€â”€ option2_rationale.md     # Why Option 2? (vs. alternatives)
    â””â”€â”€ training_data_generation.md  # How we generate examples
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Install dependencies
pip install pandas pyarrow matplotlib numpy

# Optional: for data generation
pip install google-generativeai
```

### 2. Process Datasets

```bash
# Download and convert all datasets (takes ~15-20 minutes)
python datascripts/setup_all_datasets.py

# Or process one dataset
python datascripts/setup_all_datasets.py uci_har
```

This creates standardized parquet files + manifests + debug plots.

### 3. Test Tools

```bash
# Test EDA tools on ActionSense dataset
python tools/executor.py actionsense
```

### 4. Generate Training Data (WIP)

```bash
# Set up Google Gemini API
export GEMINI_API_KEY='your-key'
# or use: gcloud auth application-default login

# Generate tool-use examples
python datascripts/actionsense/generate_training_data.py
```

---

## ğŸ“Š Available Datasets

| Dataset | Sessions | Channels | Rate | Activities | Size |
|---------|----------|----------|------|------------|------|
| **ActionSense** | 545 | 66 (joints) | 60 Hz | 23 kitchen activities | 5.0h |
| **UCI HAR** | 10,299 | 9 (accel+gyro) | 50 Hz | 6 activities | ~7.3h |
| **PAMAP2** | ~200 | 40 (3 IMUs+HR) | 100 Hz | 18 activities | ~10h |
| **MHEALTH** | ~120 | 23 (3 IMUs+ECG) | 50 Hz | 12 activities | ~2h |
| **WISDM** | ~900 | 12 (phone+watch) | 20 Hz | 18 activities | ~76h |

All converted to uniform format: `data/{dataset}/sessions/session_XXX/data.parquet`

---

## ğŸ“ Design Philosophy

### Why "Option 2"?

We evaluated three approaches:

**Option 1: Task-Specific Models on Raw Data**
- âŒ Brittle preprocessing pipelines
- âŒ Combinatorial explosion per dataset
- âŒ Undermines zero-shot story
- âœ… Best task performance when you have lots of labels

**Option 2: Frozen Tokenizer + Task Heads** (â­ This repo)
- âœ… Modular: swap heads without retraining encoder
- âœ… Fast: train heads in minutes, not hours
- âœ… Zero-shot friendly: representations transfer
- âœ… Interpretable: simple heads, LLM explanations
- âœ… Practical: fallback to Option 1 when needed

**Option 3: LLM Operates Directly on Tokens**
- âœ… Simplest UX for exploratory QA
- âŒ Weak quantitative outputs
- âŒ No calibrated predictions
- âœ… Great as EDA layer on top of Option 2

ğŸ“– **Detailed comparison:** See [`ARCHITECTURE.md`](ARCHITECTURE.md#why-option-2-vs-alternatives)

### Key Principles

1. **Real-World Time:** All reasoning uses seconds, never timesteps
2. **Frozen Foundation:** Tokenizer pretrained once, never fine-tuned per task
3. **Lightweight Heads:** Train in minutes, easy to interpret, low overfitting risk
4. **Query-Conditional:** Only decode channels user cares about (not all)
5. **Interpretable:** Every prediction comes with explanations

---

## ğŸ“ˆ Current Status

### âœ… Completed (Phase 1 Infrastructure)

- [x] Standardized data format design
- [x] 5 dataset converters with debug visualizations
- [x] EDA tool schemas (4 tools)
- [x] Real tool executors (load parquet, compute stats)
- [x] Task templates for data generation (20 templates)
- [x] Real-world time semantics throughout

### â³ In Progress (Phase 1 Training)

- [ ] Generate training examples with Gemini
- [ ] Fine-tune Llama 3B on EDA tool-use
- [ ] Validate on held-out datasets and queries

### ğŸ“‹ Planned (Phase 2)

- [ ] Design tokenizer specs (3-5 hypothetical models)
- [ ] Define task head zoo (classification, forecasting, QA)
- [ ] Create Phase 2 tool schemas
- [ ] Generate model selection training data
- [ ] Fine-tune on top of Phase 1 model

### ğŸš€ Future (Phase 3)

- [ ] Implement/adapt actual tokenizer (e.g., MOMENT-based)
- [ ] Pretrain on multi-dataset corpus
- [ ] Implement real task heads
- [ ] End-to-end benchmarking

---

## ğŸ”— Key Documents

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Full system design, data flow, rationale
- **[datascripts/README.md](datascripts/README.md)** - Dataset pipeline documentation
- **[tools/PHASE2_DESIGN.md](tools/PHASE2_DESIGN.md)** - Phase 2 tool specifications
- **[docs/option2_rationale.md](docs/option2_rationale.md)** - Why this architecture?

---

## ğŸ¤ Contributing / Using This Codebase

### For Code Agents (Claude, ChatGPT, etc.)

When working on this repository:

1. **Read [`ARCHITECTURE.md`](ARCHITECTURE.md) first** - understand the vision
2. **We're in Phase 1** - focus on EDA tool-use, not model implementation yet
3. **Real-world time always** - use seconds/Hz, never timesteps
4. **Dataset-agnostic** - code should work with any standardized dataset
5. **Verify with debug plots** - all converters generate visualizations

### For Developers

This is a research project exploring compositional foundation models. The architecture is intentionally modular to allow experimentation with:

- Different tokenizers (MOMENT, Chronos, custom)
- Different task heads (prototypes, linear, transformers)
- Different LLM orchestration strategies
- Different training data generation approaches

Feel free to adapt or extend any component while maintaining the core principles.

---

## ğŸ“ Citation & References

This project builds on ideas from:
- **Foundation models:** CLIP, SimCLR (frozen encoders + task heads)
- **Time series models:** MOMENT, Chronos, TimesFM
- **LLM tool-use:** ReAct, Toolformer, function calling
- **Design discussions:** See `docs/option2_rationale.md` for detailed analysis

---

## ğŸ“œ License

[Add your license here]

---

## ğŸ·ï¸ Branch Info

**Branch:** `tool-use-om` (tool-use omni model)
**Purpose:** Explore compositional tool-using agent architecture
**Status:** Active development (Phase 1)

**Other branches:**
- `master` - Previous work on direct end-to-end training with MOMENT/Chronos
