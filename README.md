# IMU Activity Recognition Encoder - Pretraining

**Self-supervised pretraining of a dual-branch transformer encoder for IMU-based human activity recognition across multiple datasets.**

---

## ğŸ¯ What is This?

This project implements a **pretrained encoder** for IMU (Inertial Measurement Unit) time series data that can be fine-tuned for various activity recognition tasks. The encoder learns robust representations through:

1. **Masked Autoencoding (MAE)** - Reconstructs randomly masked sensor patches
2. **Contrastive Learning** - Aligns augmented views of the same data
3. **Dual-Branch Transformer** - Captures both temporal dynamics and cross-sensor relationships

### Key Features

- âœ… **Cross-channel attention**: Models relationships between different sensors (accelerometer â†” gyroscope)
- âœ… **Variable channel support**: Handles 6-40 channels with automatic padding/masking
- âœ… **Multi-dataset pretraining**: Trains on UCI HAR, MHEALTH, PAMAP2, WISDM simultaneously
- âœ… **Physically-plausible augmentations**: Jitter, time warp, magnitude scaling, channel shuffling
- âœ… **Mixed precision training**: FP16 for ~50% memory reduction
- âœ… **Per-dataset tracking**: Monitor learning progress per dataset

---

## ğŸ—ï¸ Architecture Overview

```
Raw IMU Data (variable length, 6-40 channels)
         â†“
    Preprocessing (interpolate to 96-sample patches)
         â†“
    Patch Tokenization (2-second windows â†’ patches)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dual-Branch Transformer Encoder               â”‚
â”‚                                                 â”‚
â”‚  For each of 4 transformer blocks:            â”‚
â”‚  1. Temporal Self-Attention                    â”‚
â”‚     â””â”€ Attention over time (patch dim)        â”‚
â”‚  2. Cross-Channel Self-Attention               â”‚
â”‚     â””â”€ Attention over sensors (channel dim)   â”‚
â”‚  3. Feed-Forward Network                       â”‚
â”‚                                                 â”‚
â”‚  Output: (batch, patches, channels, d_model)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                    â†“
   Projection Head    Reconstruction Head
   (for contrastive)  (for MAE)
         â†“                    â†“
   InfoNCE Loss        MSE Loss
         â†˜                  â†™
        Combined Loss â†’ Backprop
```

**See [`PRETRAINING_FLOW.md`](PRETRAINING_FLOW.md) for detailed data flow diagrams.**

---

## ğŸ“‚ Repository Structure

```
tsfm/
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ PRETRAINING_FLOW.md                    # Complete architecture diagrams
â”œâ”€â”€ HARDCODED_CONFIG_SUMMARY.md            # Configuration documentation
â”‚
â”œâ”€â”€ data/                                  # Datasets (after processing)
â”‚   â”œâ”€â”€ uci_har/                          # UCI HAR dataset
â”‚   â”œâ”€â”€ mhealth/                          # MHEALTH dataset
â”‚   â”œâ”€â”€ pamap2/                           # PAMAP2 dataset
â”‚   â””â”€â”€ wisdm/                            # WISDM dataset
â”‚
â”œâ”€â”€ datascripts/                          # Dataset download & conversion
â”‚   â”œâ”€â”€ README.md                         # Dataset documentation
â”‚   â”œâ”€â”€ setup_all_datasets.py            # Master pipeline
â”‚   â””â”€â”€ {dataset}/
â”‚       â”œâ”€â”€ download.py                   # Download raw data
â”‚       â””â”€â”€ convert.py                    # Convert to standard format
â”‚
â”œâ”€â”€ datasets/                             # PyTorch dataset classes
â”‚   â””â”€â”€ imu_pretraining_dataset/
â”‚       â”œâ”€â”€ README.md                     # Dataset usage docs
â”‚       â”œâ”€â”€ multi_dataset_loader.py       # Multi-dataset dataloader
â”‚       â””â”€â”€ augmentations.py              # Physical augmentations
â”‚
â”œâ”€â”€ tools/                                # Model implementations
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ imu_activity_recognition_encoder/
â”‚           â”œâ”€â”€ README.md                 # Model documentation
â”‚           â”œâ”€â”€ encoder.py                # Main encoder
â”‚           â”œâ”€â”€ transformer.py            # Dual-branch transformer
â”‚           â”œâ”€â”€ config.py                 # Model configurations
â”‚           â””â”€â”€ tests/                    # Unit tests
â”‚
â””â”€â”€ training_scripts/                     # Training scripts
    â””â”€â”€ imu_tool_pretraining/
        â”œâ”€â”€ README.md                     # Training documentation
        â”œâ”€â”€ pretrain.py                   # Main training script (hard-coded config)
        â”œâ”€â”€ losses.py                     # MAE + Contrastive losses
        â””â”€â”€ config.yaml                   # Reference config (not used)
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install dependencies
pip install torch torchvision
pip install numpy scipy pandas pyarrow
pip install matplotlib  # For plotting
pip install tqdm  # For progress bars
```

### 2. Download & Process Datasets

```bash
# Download and convert all datasets (~20 minutes, ~2GB)
python datascripts/setup_all_datasets.py

# Or process individually
python datascripts/setup_all_datasets.py uci_har
python datascripts/setup_all_datasets.py mhealth
python datascripts/setup_all_datasets.py pamap2
python datascripts/setup_all_datasets.py wisdm
```

This downloads raw data, converts to standardized format, and splits into train/val/test.

### 3. Run Pretraining

```bash
cd training_scripts/imu_tool_pretraining

# Start pretraining (100 epochs, ~8-12 hours on GPU)
python pretrain.py

# Or resume from checkpoint
python pretrain.py --resume path/to/checkpoint.pt
```

Training outputs:
```
training_output/imu_pretraining/20250110_143052/
â”œâ”€â”€ config.yaml                 # Saved configuration
â”œâ”€â”€ plots/                      # PNG loss curves
â”‚   â”œâ”€â”€ overall_loss.png
â”‚   â”œâ”€â”€ loss_components.png
â”‚   â”œâ”€â”€ per_dataset_losses.png
â”‚   â”œâ”€â”€ learning_rate.png
â”‚   â”œâ”€â”€ dataset_*_detail.png
â”‚   â””â”€â”€ metrics.json
â”œâ”€â”€ latest.pt                   # Latest checkpoint
â”œâ”€â”€ best.pt                     # Best validation loss
â””â”€â”€ checkpoint_epoch_10.pt      # Periodic checkpoints
```

### 4. Monitor Training

Training automatically generates PNG plots every 10 epochs and at the end of training:

```bash
# Plots are saved to:
training_output/imu_pretraining/{timestamp}/plots/
```

Generated plots:
- `overall_loss.png` - Train/val loss curves
- `loss_components.png` - MAE loss vs Contrastive loss
- `per_dataset_losses.png` - Per-dataset loss curves (UCI HAR, MHEALTH, PAMAP2, WISDM)
- `learning_rate.png` - Learning rate schedule
- `dataset_*_detail.png` - Detailed per-dataset metrics
- `metrics.json` - Raw metrics data

---

## ğŸ“Š Datasets

| Dataset | Train | Val | Test | Channels | Rate | Activities |
|---------|-------|-----|------|----------|------|------------|
| **UCI HAR** | 7,352 | 1,470 | 1,477 | 6 (acc+gyro) | 50 Hz | 6 activities |
| **MHEALTH** | ~80 | ~20 | ~20 | 23 (3 IMUs+ECG) | 50 Hz | 12 activities |
| **PAMAP2** | ~140 | ~35 | ~25 | 40 (3 IMUs+HR) | 100 Hz | 18 activities |
| **WISDM** | ~630 | ~135 | ~135 | 6 (phone acc+gyro) | 20 Hz | 18 activities |

**Total:** ~8,200 train samples, ~1,660 val samples, ~1,660 test samples

All datasets are converted to a standardized format with:
- Variable-length time series
- Consistent channel naming
- Activity labels (not used during pretraining)
- Train/val/test splits

---

## âš™ï¸ Configuration

All hyperparameters are **hard-coded** in `pretrain.py` for easy modification:

```python
# Data
DATASETS = ['uci_har', 'mhealth', 'pamap2', 'wisdm']
PATCH_SIZE_SEC = 2.0  # 2-second patches

# Model
D_MODEL = 128
NUM_HEADS = 8
NUM_TEMPORAL_LAYERS = 4
USE_CROSS_CHANNEL = True  # Enable cross-channel attention

# Training
EPOCHS = 100
BATCH_SIZE = 32
LEARNING_RATE = 1e-4
WARMUP_EPOCHS = 10

# Loss
MAE_WEIGHT = 1.0
CONTRASTIVE_WEIGHT = 1.0
TEMPERATURE = 0.2
MASK_RATIO = 0.5  # 50% masking
```

**See [`HARDCODED_CONFIG_SUMMARY.md`](HARDCODED_CONFIG_SUMMARY.md) for details.**

To change hyperparameters, edit the constants at the top of `main()` in `pretrain.py` (lines 481-520).

---

## ğŸ§ª Model Details

### Encoder Architecture

- **Input:** Patches of shape (batch, num_patches, 96, num_channels)
- **CNN Feature Extraction:** Multi-scale convolutions [3,5,7] kernels â†’ d_model=128
- **Positional Embeddings:** Patch position + channel position
- **Transformer Blocks (Ã—4):**
  - Temporal self-attention (across patches)
  - Cross-channel self-attention (across sensors)
  - Feed-forward network (128 â†’ 512 â†’ 128)
- **Output:** (batch, num_patches, num_channels, d_model)

### Pretraining Objectives

1. **Masked Autoencoding (MAE):**
   - Randomly mask 50% of patches
   - Reconstruct masked patches from encoded representations
   - Normalized MSE loss (per-patch normalization)

2. **Contrastive Learning (InfoNCE):**
   - Apply augmentations (jitter, scale, time warp, channel shuffle)
   - Maximize agreement between original and augmented views
   - Patch-level contrastive loss across batch

### Augmentations

- **Weak:** jitter, scale, time_shift (preserve semantics)
- **Strong:** time_warp, magnitude_warp, resample (more aggressive)
- **Novel:** channel_shuffle (robustness to channel ordering)

**See [`datasets/imu_pretraining_dataset/README.md`](datasets/imu_pretraining_dataset/README.md) for augmentation details.**

---

## ğŸ“ˆ Training Tips

### Expected Performance

- Initial loss: ~2-3
- After convergence: ~0.5-1.0
- Training time: 8-12 hours on single GPU (V100/A100)

### Per-Dataset Metrics

Monitor per-dataset losses to identify:
- Which datasets are harder to learn
- Dataset imbalance issues
- Overfitting on specific datasets

### Memory Usage

- With mixed precision (FP16): ~8-12 GB GPU memory
- Batch size 32: ~10 GB
- Reduce batch size if OOM errors occur

### Debugging

If training diverges:
1. Check data loading (run tests in `datasets/imu_pretraining_dataset/`)
2. Verify augmentations aren't too aggressive
3. Reduce learning rate or increase warmup
4. Check for NaN gradients in TensorBoard

---

## ğŸ“ Fine-Tuning (Future Work)

After pretraining, the encoder can be fine-tuned for:

1. **Activity Classification:** Add linear head, fine-tune on labeled data
2. **Activity Detection:** Add segmentation head for temporal localization
3. **Anomaly Detection:** Train classifier on normal data only
4. **Transfer Learning:** Fine-tune on new datasets/activities

The pretrained weights are in `best.pt` under `model_state_dict['encoder']`.

---

## ğŸ”— Key Documents

- **[PRETRAINING_FLOW.md](PRETRAINING_FLOW.md)** - Complete architecture & data flow diagrams
- **[HARDCODED_CONFIG_SUMMARY.md](HARDCODED_CONFIG_SUMMARY.md)** - Configuration guide
- **[tools/models/imu_activity_recognition_encoder/README.md](tools/models/imu_activity_recognition_encoder/README.md)** - Model API
- **[datasets/imu_pretraining_dataset/README.md](datasets/imu_pretraining_dataset/README.md)** - Dataset details
- **[training_scripts/imu_tool_pretraining/README.md](training_scripts/imu_tool_pretraining/README.md)** - Training details

---

## ğŸ› Testing

Run unit tests:

```bash
# Test encoder
python tools/models/imu_activity_recognition_encoder/tests/test_encoder.py

# Test transformer (including cross-channel attention)
python tools/models/imu_activity_recognition_encoder/tests/test_transformer.py

# Test losses
python training_scripts/imu_tool_pretraining/losses.py

# Test augmentations
python datasets/imu_pretraining_dataset/augmentations.py
```

All tests should pass before training.

---

## ğŸ“ Recent Changes

- âœ… Implemented dual-branch transformer with cross-channel attention
- âœ… Fixed critical bugs in per-dataset tracking and normalization
- âœ… Added mixed precision training (AMP)
- âœ… Hard-coded all hyperparameters in main() for easier modification
- âœ… Added channel shuffling augmentation
- âœ… Fixed learning rate scheduler (per-batch stepping)

---

## ğŸ¤ Contributing

When working on this codebase:

1. **Test before committing:** Run all tests to ensure nothing broke
2. **Document changes:** Update relevant README files
3. **Follow conventions:** Use the existing code style
4. **Check for bugs:** Run the bug review before major changes

---

## ğŸ“œ License

[Add your license here]

---

## ğŸ·ï¸ Branch Info

**Branch:** `tool-use-om2` (renamed from tool-use-om)
**Purpose:** IMU activity recognition encoder pretraining
**Status:** Active development - pretraining infrastructure complete, ready for training
